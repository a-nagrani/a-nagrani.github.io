<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Arsha Nagrani</title>
  
  <meta name="author" content="Arsha Nagrani">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:65%;vertical-align:middle">
              <p style="text-align:center">
                <name>Arsha Nagrani</name>
		<br>
		 <a href="mailto:removethisifyouarehumanarsha@robots.ox.ac.uk", style="color:grey">anagrani at google dot com</a>
		<br>
		<br>
              </p>
              <p>I am a senior research scientist at <a href="https://ai.google/research/">Google AI Research</a>, where I work on machine learning for video understanding. I did my PhD with <a href="https://en.wikipedia.org/wiki/Andrew_Zisserman">Andrew Zisserman</a> in the <a href="http://www.robots.ox.ac.uk/~vgg/">VGG group</a> 
                at the University of Oxford, where I was fortunate enough to be funded by a <a href="https://ai.googleblog.com/2018/04/announcing-2018-google-phd-fellows-for.html">Google PhD Fellowship</a>. My thesis won the <a href="https://ellis.eu/news/ellis-phd-award-2021">ELLIS PhD award</a>.
		    </p>
		    <p>
              Before that I did my undergrad at the University of <a href="https://www.cam.ac.uk/">Cambridge</a>, where I worked with <a href="https://mi.eng.cam.ac.uk/~cipolla/">Roberto Cipolla</a> and <a href="http://www.eng.cam.ac.uk/profiles/ret26">Richard Turner</a>.</a>
              </p>
              <p>
              
               
              </p>
              <p style="text-align:center">
                <a href="Arsha_Nagrani_CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=-_2vpWwAAAAJ&hl=en&oi=ao">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/arsha-nagrani-601a726b/"> LinkedIn </a> &nbsp/&nbsp
		<a href="https://twitter.com/nagraniarsha?lang=en">Twitter</a> &nbsp/&nbsp
    <a href="https://github.com/a-nagrani"> GitHub </a> &nbsp/&nbsp
    <a href="https://www.robots.ox.ac.uk/~vgg/publications/2020/Nagrani20e/nagrani20e.pdf"> Thesis </a>
    
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/Arsha_headshot.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Arsha_headshot.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
	
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	<!---
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                2020
                  <ul>

                      <li>Submissions now open for the <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/competition2020.html">VoxCeleb Speaker Recognition Challenge</a> at Interspeech 2020!</li>
                      <li>I am spending the summer as a Visiting Researcher at <a href="https://www.wadhwaniai.org/">Wadhwani AI</a>, a non-profit in Mumbai that develop AI solutions for social good.</li>
                      <li>Submissions open for the <a href="https://www.robots.ox.ac.uk/~vgg/challenges/video-pentathlon/">Video Pentathlon</a> at CVPR 2020! Baselines, code and pre-extracted features provided.</li>
                      <li>Submissions open for the <a href="http://sightsound.org/">Sight and Sound</a> workshop at CVPR 2020!</li>
                      <li>I am once again co-organizing the <a href="https://sites.google.com/view/wicvworkshop-cvpr2020/"> Women in Computer Vision workshop </a> at CVPR 2020 in Seattle.</li>
                  </ul>
                2019 
                  <ul>
                      <li>Our <a href="https://advances.sciencemag.org/content/5/9/eaaw0736">work</a> on Chimpanzee Face Recognition was featured in the <a href="https://www.newscientist.com/article/2215359-ai-facial-recognition-software-now-works-for-wild-chimpanzees-too/">New Scientist</a> and the 
                        <a href="https://www.technologyreview.com/f/614260/ai-face-recognition-tracks-chimpanzees/">MIT Tech Review</a>. </li>
                      <li>We held the first ever <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/competition.html"> VoxCeleb Speaker Recognition Challenge </a> at Interspeech 2019 in Graz, Austria! Over 50 teams participated.</li>
                      <li>I helped organise the <a href="https://wicvworkshop.github.io/CVPR2019/index.html"> Women in Computer Vision workshop </a> at CVPR 2019 in Long Beach, Ca.</li>
                      <li>I am spending the summer at <a href="https://ai.google/research/">Google AI Research</a> in California working with <a href="http://chensun.me/">Chen Sun </a> and <a href="https://thoth.inrialpes.fr/~schmid/">Cordelia Schmid</a>.</li>

                  </ul>
                2017
                <ul>
                    <li> I received a <a href="https://ai.googleblog.com/2018/04/announcing-2018-google-phd-fellows-for.html">Google PhD Fellowship</a>.</li>
                    <li> <a href="http://www.robots.ox.ac.uk/~vgg/publications/2017/Nagrani17/nagrani17.pdf">Our paper</a> won the <a href="https://www.isca-speech.org/iscaweb/index.php/honors/awards">Best Student Paper Award</a> at Interspeech 2017! </li>
                    
                </ul>

              </p>
            </td>
          </tr>
-->
          <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <heading>Research</heading>
        
            <p>
      
              My research focuses on self-supervised and multi-modal machine learning techniques for video recognition, including the use of sound and text to learn better visual representations. 
              Recently, I have <i>also</i> become interested in computer vision for wildlife conservation. For a full list of publications please see <a href="https://scholar.google.com/citations?user=-_2vpWwAAAAJ&hl=en&oi=ao">Google Scholar</a>.
      
            </p>
          </td>
            </tr>
        </tbody></table>
	
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
         	   <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/ivsum.png" alt="ivsum" width="160" height="100">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2208.06773">
                    <papertitle> TL;DW? Summarizing Instructional Videos with Task Relevance and Cross-Modal Saliency </papertitle>
                  </a>
                  <br>
			Medhini Narasimhan, <b> Arsha Nagrani </b>, Chen Sun, Miki Rubinstein, Trevor Darrell, Anna Rohrbach, Cordelia Schmid
                  <br>
                  <em>ECCV</em>, 2022 &nbsp 
                  <br>
                  <a href="https://arxiv.org/abs/2208.06773">arXiv</a> / <a href="https://medhini.github.io/ivsum/">project page</a> / <a href="https://github.com/medhini/Instructional-Video-Summarization">code, WikiHow Summaries dataset</a>
                  <p></p>
                  <p>Creating short video summaries for instructional videos using simple heuristics. New test set released. </p>
                </td>
              </tr>
		
	   <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/avatar.png" alt="avatar" width="160" height="100">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2206.07684">
                    <papertitle> AVATAR: Unconstrained Audiovisual Speech Recognition </papertitle>
                  </a>
                  <br>
		 Valentin Gabeur*, Paul Hongsuck Seo*, <b>Arsha Nagrani*</b>, Chen Sun, Karteek Alahari, Cordelia Schmid
                  <br>
                  <em>Interspeech</em>, 2022 &nbsp 
                  <br>
                  <a href="https://arxiv.org/abs/2206.07684">arXiv</a> / <a href="https://gabeur.github.io/avatar-visspeech">project page, visSpeech dataset</a>
                  <p></p>
                  <p>Visual context (objects, actions) improves ASR performance under challenging audio conditions. </p>
                </td>
              </tr>
		
	   <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/videocc.png" alt="videocc" width="160" height="100">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2204.00679">
                    <papertitle> Learning Audio-Video Modalities from Image Captions </papertitle>
                  </a>
                  <br>
		 <b>Arsha Nagrani</b>, Paul Hongsuck Seo, Bryan Seybold, Anja Hauth, Santiago Manen, Chen Sun, Cordelia Schmid
                  <br>
                  <em>ECCV</em>, 2022 &nbsp 
                  <br>
                  <a href="https://arxiv.org/abs/2204.00679">arXiv</a> / <a href="https://github.com/google-research-datasets/videoCC-data">VideoCC dataset</a>
                  <p></p>
                  <p>Mining audiovisual clips for text captions by leveraging image-image similarity for SOTA video retrieval and captioning. </p>
                </td>
              </tr>
		     
	     <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/mv-gpt.png" alt="mvgpt" width="160" height="100">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2201.08264">
                    <papertitle> End-to-end Generative Pretraining for Multimodal Video Captioning </papertitle>
                  </a>
                  <br>
		Paul Hongsuck Seo, <b>Arsha Nagrani</b>, Anurag Arnab, Cordelia Schmid
                  <br>
                  <em>CVPR</em>, 2022 &nbsp 
                  <br>
                  <a href="https://arxiv.org/abs/2201.08264">arXiv</a> 
                  <p></p>
                  <p>New unsupervised pretraining framework for multimodal video captioning that leverages future utterances in unlabelled videos. </p>
                </td>
              </tr>
	     <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/MMCVR.png" alt="mmcvr" width="160" height="100">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://openaccess.thecvf.com/content/WACV2022/papers/Gabeur_Masking_Modalities_for_Cross-Modal_Video_Retrieval_WACV_2022_paper.pdf">
                    <papertitle> Masking Modalities for Cross-modal Video Retrieval </papertitle>
                  </a>
                  <br>
		Valentin Gabeur, <b>Arsha Nagrani</b>, Chen Sun, Karteek Alahari, Cordelia Schmid
                  <br>
                  <em>WACV</em>, 2022 &nbsp 
                  <br>
                  <a href="https://openaccess.thecvf.com/content/WACV2022/papers/Gabeur_Masking_Modalities_for_Cross-Modal_Video_Retrieval_WACV_2022_paper.pdf">PDF</a> 
                  <p></p>
                  <p>Video encoder pretraining using appearance, sound, and transcribed speech, by masking out an entire modality and predicting it using the others. </p>
                </td>
              </tr>		
	     <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/chimp-actions-compressed.gif" alt="chimpactions" width="160" height="120">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://www.science.org/doi/10.1126/sciadv.abi4883">
                    <papertitle> Automated audiovisual behavior recognition in wild primates </papertitle>
                  </a>
                  <br>
			Max Bain, <b> Arsha Nagrani </b>, Daniel Schofield, Sophie Berdugo, Joana Bessa, Jake Owens, Kimberley J. Hockings, Tetsuro Matsuzawa, Misato Hayashi, Dora Biro, Susana Carvalho, Andrew Zisserman
                  <br>
                  <em>Science Advances</em>, 2021 &nbsp 
                  <br>
                  <a href="https://www.science.org/doi/10.1126/sciadv.abi4883">PDF</a> 
                  <p></p>
                  <p>Fully automated, audio-visual pipeline to detect and track two audio-visually distinctive actions in wild chimpanzees: buttress-drumming and nut-cracking. </p>
                </td>
              </tr>
		
               <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/MBT.png" alt="mbt" width="160" height="100">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/pdf/2107.00135.pdf">
                    <papertitle> Attention Bottlenecks for Multimodal Fusion </papertitle>
                  </a>
                  <br>
                  <b>Arsha Nagrani</b>, Shan Yang, Anurag Arnab, Aren Jansen, Cordelia Schmid, Chen Sun 
                  <br>
                  <em>NeurIPS</em>, 2021 &nbsp 
                  <br>
                  <a href="https://arxiv.org/pdf/2107.00135.pdf">arXiv</a> / <a href="https://a-nagrani.github.io/mbt.html">project page</a> / <a href="https://github.com/google-research/scenic/tree/main/scenic/projects/mbt">code</a> / <a href="https://ai.googleblog.com/2022/03/multimodal-bottleneck-transformer-mbt.html", style="color: orange">google AI blog</a>
                  <p></p>
                  <p>Fully transformer based multimodal fusion model gets SOTA on video classification. Attention bottlenecks at multiple layers force cross-modal information to be condensed thereby improving performance at lower computational cost. </p>
                </td>
              </tr>
		
		
	     <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/frozen_time.png" alt="frozen_in_time" width="160" height="120">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/pdf/2104.00650.pdf">
                    <papertitle> Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval </papertitle>
                  </a>
                  <br>
                  Max Bain, <b>Arsha Nagrani</b>, Gul Varol, Andrew Zisserman
                  <br>
                  <em>ICCV</em>, 2021 &nbsp 
                  <br>
                  <a href="https://arxiv.org/pdf/2104.00650.pdf">arXiv</a> / <a href="https://github.com/m-bain/frozen-in-time">code, models</a> / <a href="https://m-bain.github.io/webvid-dataset/">WebVid dataset</a>
                  <p></p>
                  <p>End-to-end encoder for visual retrieval that uses only self-attention blocks. This allows flexible training with variable length videos and images jointly. </p>
                </td>
              </tr>
		        <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/cate.png" alt="CATE" width="160" height="120">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/pdf/2104.00616.pdf">
                    <papertitle> Composable Augmentation Encoding for Video Representation Learning </papertitle>
                  </a>
                  <br>
                  Chen Sun, <b>Arsha Nagrani</b>, Yonglong Tian, Cordelia Schmid
                  <br>
                  <em>ICCV</em>, 2021 &nbsp 
                  <br>
                  <a href="https://arxiv.org/pdf/2104.00616.pdf">arXiv</a> / <a href="https://sites.google.com/corp/brown.edu/cate-iccv2021/"> project page</a> / <a href="https://github.com/google-research/google-research/tree/master/cate"> models </a> 
                  <p></p>
                  <p> Encoding augmentations along with data views gives SOTA on video self-supervised learning benchmarks. </p>
                </td>
              </tr>
        <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/avsync.png" alt="av_sync" width="160" height="120">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://www.robots.ox.ac.uk/~vgg/research/avs/">
                    <papertitle> Audio-Visual Synchronisation in the Wild </papertitle>
                  </a>
                  <br>
                  Honglie Chen, Weidi Xie, Triantafyllos Afouras, <b>Arsha Nagrani</b>, Andrea Vedaldi, Andrew Zisserman
                  <br>
                  <em>BMVC</em>, 2021 &nbsp 
                  <br>
                  <a href="https://arxiv.org/abs/2112.04432">arXiv</a> / <a href="https://www.robots.ox.ac.uk/~vgg/research/avs/">VGGSound-sync data, project page</a> 
                  <p></p>
                  <p>Transformer model for audio-visual synchronization works well on non-speech classes in the wild.</p>
                </td>
              </tr>
		
        <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/vggss_crop.gif" alt="localize_sound" width="160" height="120">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/pdf/2104.02691.pdf">
                    <papertitle> Localizing Visual Sounds the Hard Way </papertitle>
                  </a>
                  <br>
                  Honglie Chen, Weidi Xie, Triantafyllos Afouras, <b>Arsha Nagrani</b>, Andrea Vedaldi, Andrew Zisserman
                  <br>
                  <em>CVPR</em>, 2021 &nbsp 
                  <br>
                  <a href="https://arxiv.org/pdf/2104.02691.pdf">arXiv</a> / <a href="https://www.robots.ox.ac.uk/~vgg/research/lvs/">VGG-SS dataset, project page</a> 
                  <p></p>
                  <p>Localizes sounding objects without any supervision using hard negative mining from within the image. Gives SOTA on Flickr SoundNet and a new VGG-SS dataset. </p>
                </td>
              </tr>
	<tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/temporal_context.png" alt="mtcn" width="160" height="110">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://ekazakos.github.io/MTCN-project/">
                    <papertitle>With a Little Help from my Temporal Context: Multimodal Egocentric Action Recognition </papertitle>
                  </a>
                  <br>
		   Evangelis Kazakos, Jaesung Huh, <b>Arsha Nagrani</b>, Andrew Zisserman, Dima Damen
                  <br>
                  <em>BMVC</em>, 2021 &nbsp 
                  <br>
                  <a href="https://arxiv.org/abs/2111.01024">arXiv</a> / 
		  <a href="https://github.com/ekazakos/MTCN">code, models</a> /
		  <a href="https://ekazakos.github.io/MTCN-project/">project page</a> 
                  <p></p>
		  <p>Uses a language model to learn a sequence of actions as temporal context for egocentric action recognition.</p>
                  
              </tr>
				
	<tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/vis_cont_dialogue.png" alt="disent" width="160" height="110">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/pdf/2012.05710.pdf">
                    <papertitle> Look Before you Speak: Visually Contextualized Utterances </papertitle>
                  </a>
                  <br>
                  Paul Hongsuck Seo, <b>Arsha Nagrani</b>, Cordelia Schmid
                  <br>
                  <em>CVPR</em>, 2021 &nbsp 
                  <br>
                  <a href="https://arxiv.org/pdf/2012.05710.pdf">arXiv</a> / <a href="https://google.github.io/look-before-you-speak/">project page</a>
                  <p></p>
                  <p>Predicting future utterances in a video based on previous dialogue and video frames without manual labels gives SOTA on standard QA datasets. </p>
                </td>
              </tr>
		
	<tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/slowfastaudio.png" alt="disent" width="160" height="110">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/pdf/2103.03516.pdf">
                    <papertitle> Slow-Fast Auditory Streams For Audio Recognition </papertitle>
                  </a>
                  <br>
			Evangelis Kazakos, <b>Arsha Nagrani</b>, Andrew Zisserman, Dima Damen
                  <br>
                  <em>ICASSP</em>, 2021 &nbsp <font color="red"><strong>(Outstanding Paper Award)</strong></font>
                  <br>
                  <a href="https://arxiv.org/pdf/2103.03516.pdf">arXiv</a> / 
		  <a href="https://github.com/ekazakos/auditory-slow-fast">code, models</a> /
		  <a href="https://ekazakos.github.io/auditoryslowfast/">project page</a> 
                  <p></p>
                  <p>Two stream audio recognition models that gets SOTA on VGG-Sound and EPIC-Kitchens-100. 
              </tr>
		
		
		<tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/voxmovies.png" alt="disent" width="160" height="110">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/pdf/2010.15716.pdf">
                    <papertitle> Playing a Part: Speaker Verification at the Movies </papertitle>
                  </a>
                  <br>
                  Andrew Brown*, Jaesung Huh*, <b>Arsha Nagrani*</b>, Joon Son Chung, Andrew Zisserman
                  <br>
                  <em>ICASSP</em>, 2021 &nbsp 
                  <br>
                  <a href="https://arxiv.org/pdf/2010.15716.pdf">arXiv</a> / 
		  <a href="https://www.robots.ox.ac.uk/~vgg/data/voxmovies/">VoxMovies dataset, project page</a> 
                  <p></p>
                  <p>Investigate the performance of speaker recognition in movies, where often actors intentionally disguise their voice to play a character.
              </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/CMD.png" alt="disent" width="160" height="110">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2005.04208">
                <papertitle> Condensed Movies: Story Based Retrieval with Contextual Embeddings </papertitle>
              </a>
              <br>
               Max Bain, <b>Arsha Nagrani</b>, Andrew Brown, Andrew Zisserman 
              <br>
              <em>ACCV</em>, 2020 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="http://www.robots.ox.ac.uk/~vgg/research/condensed-movies/">project page, CMD dataset</a> / <a href="https://www.robots.ox.ac.uk/~vgg/research/condensed-movies/challenge.html", style="color: orange"> challenge</a> 
              <p></p>
              <p>A large-scale story understanding dataset that contains the <b>key scenes</b> from movies with semantic captions. Basis of the <a href="https://www.robots.ox.ac.uk/~vgg/research/condensed-movies/challenge.html", style="color: orange">CMD Challenge </a> at ICCV 2021.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/voxconverse.png" alt="disent" width="160" height="110">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2007.01216.pdf">
                <papertitle> Spot the conversation: speaker diarisation in the wild   </papertitle>
              </a>
              <br>
              Joon Son Chung*, Jaesung Huh*, <b>Arsha Nagrani*</b>, Triantafyllos Afouras, Andrew Zisserman
              <br>
              <em>INTERSPEECH</em>, 2020 
              <br>
              <a href="http://www.robots.ox.ac.uk/~vgg/data/voxconverse/">project page, VoxConverse dataset</a> / <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/competition.html", style="color: orange"> challenge</a> 
              <p></p>
              <p>Breaking up multispeaker videos into "who spoke when". Based on this work we are hosting a new <i>speaker diarisation</i> track 
                at the <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/competition.html", style="color: orange"> VoxCeleb Speaker Recognition Challenge. </a></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/weakly_sup_ad-1.png" alt="disent" width="160" height="110">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2007.10703">
                <papertitle> Uncertainty-Aware Weakly Supervised Action Detection from Untrimmed Videos </papertitle>
              </a>
              <br>
               Anurag Arnab, Chen Sun, <b>Arsha Nagrani</b>, Cordelia Schmid 
              <br>
              <em>ECCV</em>, 2020 
              <br>
              <a href="https://arxiv.org/abs/2007.10703">arXiv</a> 
              <p></p>
              <p>Action localisation in movies using video level labels only.</p>
            </td>
          </tr>
          
          
              
              <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/s2a.gif" alt="disent" width="160" height="110">
            </td>
            <td width="75%" valign="middle">
              <a href="http://www.robots.ox.ac.uk/~vgg/publications/2020/Nagrani20/nagrani20.pdf">
                <papertitle> Speech2Action: Cross-modal Supervision for Action Recognition </papertitle>
              </a>
              <br>
               <b>Arsha Nagrani</b>, Chen Sun, David Ross, Rahul Sukthankar, Cordelia Schmid, Andrew Zisserman 
              <br>
              <em>CVPR</em>, 2020 
              <br>
              <a href="https://www.robots.ox.ac.uk/~vgg/research/speech2action/">project page, data</a> /
              <a href="https://docs.google.com/presentation/d/1ZJLwtZ1xt9OTtPD2NmzucVF-Ymk3vG7eCEd8EcQQ_-8/edit?usp=sharing">slides</a> 
              <p></p>
              <p>Action recognition in movies using the speech alone.</p>
            </td>
          </tr>

            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/disentangled.png" alt="disent" width="170" height="150">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2002.08742">
                    <papertitle>Disentangled Speech Embeddings using Cross-modal Self-supervision </papertitle>
                  </a>
                  <br>
             <b>Arsha Nagrani*</b>, Joon Son Chung*, Samuel Albanie*, Andrew Zisserman 
                  <br>
                  <em>ICASSP</em>, 2020 
                  <br>
                  <a href="https://www.robots.ox.ac.uk/~vgg/research/cross-modal-disentanglement/">project page</a> /
            <a href="https://github.com/joonson/syncnet_trainer">some code</a> 
                  <p></p>
                  <p>Disentanglement of speech embeddings into content and identity with only accompanying facetrack as supervision. Based on this work we are hosting a new <i>self-supervised</i> track 
                    at the <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/competition.html", style="color: orange"> VoxCeleb Speaker Recognition Challenge. </a>
                  </p>
                </td>
              </tr>


            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/spID.png" alt="vox" width="170" height="150">
                </td>
                <td width="75%" valign="middle">
                  <a href="http://www.robots.ox.ac.uk/~vgg/publications/2019/Nagrani19/nagrani19.pdf">
                    <papertitle>Voxceleb: Large-scale speaker verification in the wild </papertitle>
                  </a>
                  <br>
            <b>Arsha Nagrani</b>, Joon Son Chung, Weidi Xie, Andrew Zisserman 
                  <br>
                  <em>Computer Speech and Language</em>, 2020 
                  <br>
            <a href="http://www.robots.ox.ac.uk/~vgg/research/speakerID/">project page, data </a> /
                  <a href="https://github.com/WeidiXie/VGG-Speaker-Recognition">code & models</a> /
                  <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/competition.html", style="color: orange"> challenge</a> 
                  <p></p>
                  <p>Overview of the VoxCeleb1 and VoxCeleb2 datasets including various updates and splits, and new models for speaker recognition.</p>
                </td>
              </tr>

            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/CCR.gif" alt="ccr" width="170" height="130">
                </td>
                <td width="75%" valign="middle">
                  <a href="http://www.robots.ox.ac.uk/~vgg/publications/2019/Bain19/bain19.pdf">
                    <papertitle>Count, Crop and Recognise: Fine-Grained Recognition in the Wild  </papertitle>
                  </a>
                  <br> Max Bain, <b>Arsha Nagrani</b>, Daniel Schofield, Andrew Zisserman
                  <br>
                  <em>ICCV Workshops</em>, 2019 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                  <br>
                  <a href="http://www.robots.ox.ac.uk/~vgg/research/ccr/">project page</a> / <a href="http://www.robots.ox.ac.uk/~vgg/research/ccr/files/ICCV_Pres.pptx">slides</a>
                </br>
          
                  <p></p>
                  <p>Recognition of wild chimpanzees using full body and full frame CNNs methods. We also release an 'in the wild' video chimpanzee recognition dataset. 	  
            </p>
                </td>
              </tr>
              
              
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/chimpanzees.gif" alt="chimpanzees-facial-recognition" width="170" height="130">
            </td>
            <td width="75%" valign="middle">
              <a href="https://advances.sciencemag.org/content/5/9/eaaw0736">
                <papertitle>Chimpanzee face recognition from videos in the wild using deep learning</papertitle>
              </a>
              <br> Daniel Schofield*, <b>Arsha Nagrani*</b>, Andrew Zisserman, Misato Hayashi, Tetsuro Matsuzawa, Dora Biro, Susana Carvalho
              <br>
              <em>Science Advances</em>, 2019
              <br>
              <a href="https://www.robots.ox.ac.uk/~vgg/research/ChimpanzeeFaces/">project page</a> 
            </br>
              
		      Press:
		      <font color="orange">
				  <a href="https://www.newscientist.com/article/2215359-ai-facial-recognition-software-now-works-for-wild-chimpanzees-too/", style="color: orange">New Scientist</a>,
				  <a href="https://www.technologyreview.com/f/614260/ai-face-recognition-tracks-chimpanzees/", style="color: orange">MIT Tech Review</a>, 
				  <a href="https://techxplore.com/news/2019-09-artificial-intelligence-primate-wild.html", style="color: orange">TechXplore</a>, 
				  <a href="https://www.verdict.co.uk/chimpanzees-facial-recognition/", style="color: orange">Verdict</a>, 
				  <a href="https://www.digitaltrends.com/cool-tech/facial-recognition-chimpanzee/", style="color: orange">Digital Trends</a>, 
				  <a href="https://www.eng.ox.ac.uk/news/artificial-intelligence-used-to-recognise-primate-faces-in-the-wild/", style="color: orange">Oxford News</a> 
			  </font>
			  <br>
              <p></p>
              <p>Face detection, tracking, and recognition of wild chimpanzees from long-term video records using deep CNNs. We also show a brief application for social network analysis.
			  </p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/TBW.png" alt="clean-usnob" width="170" height="140">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/1908.08498.pdf">
                <papertitle>EPIC-Fusion: Audio-Visual Temporal Binding for Egocentric Action Recognition</papertitle>
              </a>
              <br>
			  Evangelos Kazakos, <b>Arsha Nagrani</b>, Andrew Zisserman, Dima Damen
              <br>
              <em>ICCV</em>, 2019
              <br>
			  <a href="https://ekazakos.github.io/TBN/">project page</a> /
              <a href="https://www.youtube.com/watch?time_continue=177&v=VzoaKsDvv1o">video</a> /
              <a href="https://github.com/ekazakos/temporal-binding-network">code and models</a> 
              <br>
              <p></p>
              <p>We propose a novel architecture for combining modalities in videos for action recognition, by using a temporal window to allow a range of temporal offsets.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/CE.png" alt="clean-usnob" width="170" height="150">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/1907.13487.pdf">
                <papertitle>Use What You Have: Video Retrieval Using Representations From Collaborative Experts</papertitle>
              </a>
              <br>
              Yang Liu*, Samuel Albanie*, <b>Arsha Nagrani*</b>, Andrew Zisserman
              <br>
              <em>BMVC</em>, 2019
              <br>
			  <a href="https://www.robots.ox.ac.uk/~vgg/research/collaborative-experts/">project page</a> /
              <a href="https://github.com/albanie/collaborative-experts">code & models</a> /
              <a href="https://www.robots.ox.ac.uk/~vgg/challenges/video-pentathlon/", style="color: orange"> challenge</a> 

              <p></p>
              <p>We fuse the information from different embeddings experts for the task of video retrieval - achieving SOTA results on 5 different datasets. This work is also the basis 
                for the <a href="https://www.robots.ox.ac.uk/~vgg/challenges/video-pentathlon/">Video Pentathlon</a> at CVPR 2020. 
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/spID.png" alt="clean-usnob" width="170" height="150">
            </td>
            <td width="75%" valign="middle">
              <a href="http://www.robots.ox.ac.uk/~vgg/publications/2019/Xie19a/xie19a.pdf">
                <papertitle>Utterance-level Aggregation For Speaker Recognition In The Wild  </papertitle>
              </a>
              <br>
			  Weidi Xie, <b>Arsha Nagrani</b>, Joon Son Chung, Andrew Zisserman 
              <br>
              <em>ICASSP</em>, 2019 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
			  <a href="http://www.robots.ox.ac.uk/~vgg/research/speakerID/">project page</a> /
              <a href="https://github.com/WeidiXie/VGG-Speaker-Recognition">code & models</a> 
              <p></p>
              <p>A NetVlad layer in a deep CNN works well for speaker recognition on long noisy speech utterances.</p>
            </td>
          </tr>

          <tr onmouseout="acm_stop()" onmouseover="acm_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='acm_image'><img src='images/acm-after.png' alt="clean-usnob" width="170" height="150"></div>
                <img src='images/acm-before.png' alt="clean-usnob" width="170" height="150">
              </div>
              <script type="text/javascript">
                function acm_start() {
                  document.getElementById('acm_image').style.opacity = "1";
                }

                function acm_stop() {
                  document.getElementById('acm_image').style.opacity = "0";
                }
                acm_stop()
              </script>
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/1808.05561.pdf">
                <papertitle>Emotion Recognition in Speech using Cross-Modal Transfer in the Wild  </papertitle>
              </a>
              <br>
			  Samuel Albanie*, <b>Arsha Nagrani*</b>, Andrea Vedaldi, Andrew Zisserman
              <br>
              <em>ACM Multimedia</em>, 2018 
              <br>
              <a href="http://www.robots.ox.ac.uk/~vgg/research/cross-modal-emotions/">project page</a> /
              <a href="https://github.com/albanie/mcnCrossModalEmotions">code</a> 
              <p></p>
              <p>We use the redundant (common) signal in both audio (speech) and vision (faces) to learn speech representations for emotion recognition without manual supervision.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/vox2.png" alt="safs_small" width="170" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/1806.05622.pdf">
                <papertitle>VoxCeleb2: Deep Speaker Recognition </papertitle>
              </a>
              <br>
              Joon Son Chung*, <b>Arsha Nagrani*</b>, Andrew Zisserman 
              <br>
              <em>INTERSPEECH</em>, 2018
              <br>
			  <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/">data</a> 
              <p>Speaker Recognition in the Wild using deep CNNs. The VoxCeleb datasets are also used integrally in the 
				  <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/competition.html", style="color: orange"> VoxCeleb Speaker Recognition Challenge. </a></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/jointembedding.png" alt="fast-texture" width="170" height="120">
            </td>
            <td width="75%" valign="middle">
              <a href="http://www.robots.ox.ac.uk/~vgg/publications/2018/Nagrani18c/nagrani18c.pdf">
                <papertitle>Learnable PINs: Cross-Modal Embeddings for Person Identity  </papertitle>
              </a>
              <br>
              <b>Arsha Nagrani*</b>, Samuel Albanie*, Andrew Zisserman 
              <br>
              <em>ECCV</em>, 2018
              <br>
              <a href="http://www.robots.ox.ac.uk/~vgg/research/LearnablePins/">project page</a> 
              <p>We learn joint embedding of faces and voices using cross-modal self-supervision from YouTube videos. 
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/seeingvoices.gif" alt="prl" width="170" height="120">
            </td>
            <td width="75%" valign="middle">
              <a href="http://www.robots.ox.ac.uk/~vgg/publications/2018/Nagrani18a/nagrani18a.pdf">
                <papertitle>Seeing Voices and Hearing Faces: Cross-modal biometric matching  </papertitle>
              </a>
              <br>
              <b>Arsha Nagrani</b>, Samuel Albanie, Andrew Zisserman
              <br>
              <em>CVPR</em>, 2018 &nbsp <font color="red"><strong>(Spotlight)</strong></font>
              <br>
              <a href="http://www.robots.ox.ac.uk/~vgg/research/CMBiometrics/">project page</a> / 
			  <a href="https://www.youtube.com/watch?time_continue=4&v=AJt993-VGsk"> video </a> / 
			  <a href="http://www.robots.ox.ac.uk/~vgg/blog/seeing-voices-and-hearing-faces.html"> blog post </a> 
              <p> Can you recognise someoneâ€™s face if you have only heard their voice? Or recognise their voice if you have only seen their face? </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/sherlock.gif" alt="blind-date" width="170" height="110">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.robots.ox.ac.uk/~vgg/publications/2017/Nagrani17b/nagrani17b.pdf">
                <papertitle>From Benedict Cumberbatch to Sherlock Holmes: Character Identification in TV series without a Script  </papertitle>
              </a>
              <br>
              <b>Arsha Nagrani</b>, Andrew Zisserman
              <br>
              <em>BMVC</em>, 2017 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
			  <br>
			  <a href="http://www.robots.ox.ac.uk/~vgg/research/Sherlock/">project page</a> 
              <p></p>
            </td>
          </tr>

	   <tr onmouseout="vox1_stop()" onmouseover="vox1_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='vox1_image'><img src='images/vox1-after.png' alt="clean-usnob" width="170" height="145"></div>
                <img src='images/voxceleb1.png' alt="clean-usnob" width="170" height="145">
              </div>
              <script type="text/javascript">
                function vox1_start() {
                  document.getElementById('vox1_image').style.opacity = "1";
                }

                function vox1_stop() {
                  document.getElementById('vox1_image').style.opacity = "0";
                }
                vox1_stop()
              </script>
            </td>
            <td width="75%" valign="middle">
              <a href="http://www.robots.ox.ac.uk/~vgg/publications/2017/Nagrani17/nagrani17.pdf">
                <papertitle>VoxCeleb: a large-scale speaker identification dataset  </papertitle>
              </a>
              <br>
              <b>Arsha Nagrani*</b>, Joon Son Chung*, Andrew Zisserman
              <br>
              <em>INTERSPEECH</em>, 2017 &nbsp <font color="red"><strong>(Oral Presentation, Best Student Paper Award)</strong></font>
			  <br>
			  <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/">data</a> / 
			  <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/competition.html">challenge</a> 
              <p>We use face recognition and active speaker detection to automatically create a large scale speaker identification dataset from YouTube videos.</p>
            </td>
          </tr>
        </tbody></table>
       
<!--- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
        <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Teaching/Invited Talks</heading>
            <p>
            "Multimodality for Video Understanding", <a href="https://sites.google.com/view/aisummerschool2020">Google Research India AI Summer School</a>, 2020 [<a href="https://docs.google.com/presentation/d/1J4AVI9S1KBFZyGiBdFJn64k2BRFhlRRngd8r262Xo8g/edit?usp=sharing">slides</a>] </br>
            "Learning joint representations for visual and language tasks", <a href="https://multimodal-knowledge-discovery.github.io/">Online Multimodal Knowledge Discovery Tutorial</a>, ICDM 2020
            "Applications of Machine Learning", Oxford University MPLS DTC on Statistics and Data Mining, 2020 [<a href="https://docs.google.com/presentation/d/1XgNYFLNwlODst7LiFuaof2F9uuNcv3ZdwdiyXrCT1-A/edit?usp=sharing">slides</a>]
            </p>
          </td>
        </tr>
      </tbody></table>
 --> 
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
            </td>
          </tr>
	<tr>
              <td width:100%;vertical-align:middle">
	<strong>  Area Chair </strong>: CVPR23, ICCV23
	<br>
	<strong> Reviewer </strong>: CVPR, ECCV, ICCV, BMVC, NeurIps, ICML, AAAI, IEEE Triple Access	
          </td>
            </tr>
				   
          <tr>
	
       <td width="100%" valign="center">
	 <strong> Workshop/Tutorial Organization </strong>: 
	</br>
	</br>
	<strong> Sight and Sound </strong> Workshop @ CVPR 
        </br>
        [2020-2022]
	<a href="https://sightsound.org/">website</a> 												              
       </br>
	 </br>
        <strong> VoxSRC: </strong> VoxCeleb Speaker Recognition Challenge @ INTERSPEECH 
        </br>
        [2021]
<a href="https://arxiv.org/pdf/2201.04583.pdf">report</a> / 	
	<a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/competition2021.html">challenge</a> /
	<a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/interspeech2021.html">workshop</a> /
        <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/index.html">data</a>													              
       </br>
        [2020]
        <a href="https://arxiv.org/pdf/2012.06867.pdf">report</a> /
	<a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/competition2020.html">challenge</a> /
	<a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/interspeech2020.html">workshop</a> /
        <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/index.html">data</a>													              
       </br>										 
        [2019]
        <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/files/VoxSRC19.pdf">report</a> /
	      <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/competition2019.html">challenge</a> /
	      <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/interspeech2019.html">workshop</a> /
	      <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/index.html">data</a>													               </br>										 
	      </br>
            <strong> The End-of-End-to-End: </strong>  A Video Understanding Pentathlon @ CVPR 2020
            </br>
            <a href="https://arxiv.org/pdf/2008.00744.pdf">report</a> /
            <a href="https://www.robots.ox.ac.uk/~vgg/challenges/video-pentathlon/challenge.html">challenge</a> /
            <a href="https://www.robots.ox.ac.uk/~vgg/challenges/video-pentathlon/">workshop</a> 	/
            <a href="https://www.youtube.com/watch?v=L2rff2mu1gE">recording</a> 	
            </br>										 
            </br>
        <strong> WICV: </strong> Women in Computer Vision Workshop @ CVPR
      </br>
      [2020]
      <a href="https://sites.google.com/view/wicvworkshop-cvpr2020/">website</a> /
      <a href="https://twitter.com/wicvworkshop?lang=en">twitter</a>													     									 
      </br>
        [2019]
	      <a href="http://openaccess.thecvf.com/content_CVPRW_2019/html/WiCV/Amerini_WiCV_2019_The_Sixth_Women_In_Computer_Vision_Workshop_CVPRW_2019_paper.html">report</a> /
	      <a href="https://wicvworkshop.github.io/CVPR2019/index.html">website</a> /
	      <a href="https://twitter.com/wicvworkshop?lang=en">twitter</a>													         </br>										 
        </br>
        </td>
        </tr>							
								
								
<!-- 	
          <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/sightsound.png" alt="voxsrc" width="150" height="50">
        </td>
       <td width="75%" valign="center">
	      <strong> Sight and Sound </strong> Workshop @ CVPR 
        </br>
        [2020-2022]
	<a href="https://sightsound.org/">website</a> 												              
       </br>
        </td>
        </tr>
          <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/voxsrc.png" alt="voxsrc" width="150" height="50">
        </td>
       <td width="75%" valign="center">
	      <strong> VoxSRC: </strong> VoxCeleb Speaker Recognition Challenge @ INTERSPEECH 
        </br>
        [2021]
<a href="https://arxiv.org/pdf/2201.04583.pdf">report</a> / 	
	<a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/competition2021.html">challenge</a> /
	<a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/interspeech2021.html">workshop</a> /
        <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/index.html">data</a>													              
       </br>
        [2020]
        <a href="https://arxiv.org/pdf/2012.06867.pdf">report</a> /
	<a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/competition2020.html">challenge</a> /
	<a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/interspeech2020.html">workshop</a> /
        <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/index.html">data</a>													              
       </br>										 
        [2019]
        <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/files/VoxSRC19.pdf">report</a> /
	      <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/competition2019.html">challenge</a> /
	      <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/interspeech2019.html">workshop</a> /
	      <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/index.html">data</a>													               </br>										 
	      </br>

        </td>
        </tr>
          <tr>
            <th style="padding:20px;width:25%;vertical-align:middle;text-align: center">
                      <img src="images/video-pent-logo.svg" alt="video-pent" width="100" height="60" class="center">
            </th>
           <td width="75%" valign="center">
            <strong> The End-of-End-to-End: </strong>  A Video Understanding Pentathlon @ CVPR 2020
            </br>
            <a href="https://arxiv.org/pdf/2008.00744.pdf">report</a> /
            <a href="https://www.robots.ox.ac.uk/~vgg/challenges/video-pentathlon/challenge.html">challenge</a> /
            <a href="https://www.robots.ox.ac.uk/~vgg/challenges/video-pentathlon/">workshop</a> 	/
            <a href="https://www.youtube.com/watch?v=L2rff2mu1gE">recording</a> 	
            </br>										 
            </br>
    
            </td>
            </tr>
        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/wicv.png" alt="wicv" width="150" height="50">
      </td>
      <td width="75%" valign="center">
        <strong> WICV: </strong> Women in Computer Vision Workshop @ CVPR
      </br>
      [2020]
      <a href="https://sites.google.com/view/wicvworkshop-cvpr2020/">website</a> /
      <a href="https://twitter.com/wicvworkshop?lang=en">twitter</a>													     									 
      </br>
        [2019]
	      <a href="http://openaccess.thecvf.com/content_CVPRW_2019/html/WiCV/Amerini_WiCV_2019_The_Sixth_Women_In_Computer_Vision_Workshop_CVPRW_2019_paper.html">report</a> /
	      <a href="https://wicvworkshop.github.io/CVPR2019/index.html">website</a> /
	      <a href="https://twitter.com/wicvworkshop?lang=en">twitter</a>													         </br>										 
        </br>
        </td>
        </tr> -->
<!--         <tr>
            <td style="padding:20px;width:25%;vertical-align:middle;text-align: center">
                <img src="images/reviewer.jpg" alt="review" width="150" height="70" >
      </td>
          <td>
              <strong> Reviewer </strong>: CVPR, ECCV, ICCV, BMVC, NeurIps, ICML, AAAI, IEEE Triple Access
              <br>
		 <strong>  Area Chair </strong>: CVPR23, ICCV23

            </td>
          </tr> -->
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                 <a href="https://jonbarron.info/">This</a> guy is good at website design.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
