<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Arsha Nagrani</title>
  
  <meta name="author" content="Arsha Nagrani">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:65%;vertical-align:middle">
              <p style="text-align:center">
                <name>Arsha Nagrani</name>
		<br>
		 <a href="mailto:removethisifyouarehumanarsha@robots.ox.ac.uk", style="color:grey">anagrani at google dot com</a>
		<br>
		<br>
              </p>
              <p>I am a Research Scientist at <a href="https://ai.google/research/">Google AI Research</a>, focused on machine learning for video understanding. I completed my PhD with <a href="https://en.wikipedia.org/wiki/Andrew_Zisserman">Andrew Zisserman</a> in the <a href="http://www.robots.ox.ac.uk/~vgg/">VGG group</a>, 
                with a particular focus on the intersection of computer vision and speech technology. I was fortunate enough to be funded by a <a href="https://ai.googleblog.com/2018/04/announcing-2018-google-phd-fellows-for.html">Google PhD Fellowship</a>.
		    </p>
		    <p>
              Before that I did my undergrad at the Uni. of <a href="https://www.cam.ac.uk/">Cambridge</a>, where I worked with <a href="https://mi.eng.cam.ac.uk/~cipolla/">Roberto Cipolla</a> and <a href="http://www.eng.cam.ac.uk/profiles/ret26">Richard Turner</a>.</a>
              </p>
              <p>
              
               
              </p>
              <p style="text-align:center">
                <a href="data/ArshaNagrani-CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=-_2vpWwAAAAJ&hl=en&oi=ao">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/arsha-nagrani-601a726b/"> LinkedIn </a> &nbsp/&nbsp
		<a href="https://twitter.com/nagraniarsha?lang=en">Twitter</a> &nbsp/&nbsp
    <a href="https://github.com/a-nagrani"> GitHub </a> &nbsp/&nbsp
    <a href="https://www.robots.ox.ac.uk/~vgg/publications/2020/Nagrani20e/nagrani20e.pdf"> Thesis </a>
    
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/Arsha_headshot.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Arsha_headshot.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
	
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	<!---
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                2020
                  <ul>

                      <li>Submissions now open for the <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/competition2020.html">VoxCeleb Speaker Recognition Challenge</a> at Interspeech 2020!</li>
                      <li>I am spending the summer as a Visiting Researcher at <a href="https://www.wadhwaniai.org/">Wadhwani AI</a>, a non-profit in Mumbai that develop AI solutions for social good.</li>
                      <li>Submissions open for the <a href="https://www.robots.ox.ac.uk/~vgg/challenges/video-pentathlon/">Video Pentathlon</a> at CVPR 2020! Baselines, code and pre-extracted features provided.</li>
                      <li>Submissions open for the <a href="http://sightsound.org/">Sight and Sound</a> workshop at CVPR 2020!</li>
                      <li>I am once again co-organizing the <a href="https://sites.google.com/view/wicvworkshop-cvpr2020/"> Women in Computer Vision workshop </a> at CVPR 2020 in Seattle.</li>
                  </ul>
                2019 
                  <ul>
                      <li>Our <a href="https://advances.sciencemag.org/content/5/9/eaaw0736">work</a> on Chimpanzee Face Recognition was featured in the <a href="https://www.newscientist.com/article/2215359-ai-facial-recognition-software-now-works-for-wild-chimpanzees-too/">New Scientist</a> and the 
                        <a href="https://www.technologyreview.com/f/614260/ai-face-recognition-tracks-chimpanzees/">MIT Tech Review</a>. </li>
                      <li>We held the first ever <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/competition.html"> VoxCeleb Speaker Recognition Challenge </a> at Interspeech 2019 in Graz, Austria! Over 50 teams participated.</li>
                      <li>I helped organise the <a href="https://wicvworkshop.github.io/CVPR2019/index.html"> Women in Computer Vision workshop </a> at CVPR 2019 in Long Beach, Ca.</li>
                      <li>I am spending the summer at <a href="https://ai.google/research/">Google AI Research</a> in California working with <a href="http://chensun.me/">Chen Sun </a> and <a href="https://thoth.inrialpes.fr/~schmid/">Cordelia Schmid</a>.</li>

                  </ul>
                2017
                <ul>
                    <li> I received a <a href="https://ai.googleblog.com/2018/04/announcing-2018-google-phd-fellows-for.html">Google PhD Fellowship</a>.</li>
                    <li> <a href="http://www.robots.ox.ac.uk/~vgg/publications/2017/Nagrani17/nagrani17.pdf">Our paper</a> won the <a href="https://www.isca-speech.org/iscaweb/index.php/honors/awards">Best Student Paper Award</a> at Interspeech 2017! </li>
                    
                </ul>

              </p>
            </td>
          </tr>
-->
          <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <heading>Research</heading>
        
            <p>
      
              My research focuses on self-supervised and multi-modal machine learning techniques for video recognition, including the use of sound and text to learn better visual representations. Recently, I have <i>also</i> become interested in computer vision for wildlife conservation. 
      
            </p>
          </td>
            </tr>
        </tbody></table>
	
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/vis_cont_dialogue.png" alt="disent" width="160" height="110">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/pdf/2012.05710.pdf">
                    <papertitle> Look Before you Speak: Visually Contextualized Utterances </papertitle>
                  </a>
                  <br>
                  Paul Hongsuck Seo, Arsha Nagrani, Cordelia Schmid
                  <br>
                  <em>arXiv</em>, 2020 &nbsp 
                  <br>
                  <a href="https://arxiv.org/pdf/2012.05710.pdf">arXiv</a> /
                  <a href="data/Seo20.bib">bibtex</a>
                  <p></p>
                  <p>Predicting future utterances in a video based on previous dialogue and video frames without manual labels gives SOTA on standard QA datasets. </p>
                </td>
              </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/CMD.png" alt="disent" width="160" height="110">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2005.04208">
                <papertitle> Condensed Movies: Story Based Retrieval with Contextual Embeddings </papertitle>
              </a>
              <br>
               Max Bain, Arsha Nagrani, Andrew Brown, Andrew Zisserman 
              <br>
              <em>ACCV</em>, 2020 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="http://www.robots.ox.ac.uk/~vgg/research/condensed-movies/">project page, data</a> /
              <a href="data/Bain20.bib">bibtex</a>
              <p></p>
              <p>A large-scale story understanding dataset that contains the <b>key scenes</b> from movies with semantic captions.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/voxconverse.png" alt="disent" width="160" height="110">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2007.01216.pdf">
                <papertitle> Spot the conversation: speaker diarisation in the wild   </papertitle>
              </a>
              <br>
              Joon Son Chung*, Jaesung Huh*, Arsha Nagrani*, Triantafyllos Afouras, Andrew Zisserman
               
  
              <br>
              <em>INTERSPEECH</em>, 2020 
              <br>
              <a href="http://www.robots.ox.ac.uk/~vgg/data/voxconverse/">project page, data</a> /
              <a href="data/Chung20.bib">bibtex</a>
              <p></p>
              <p>Breaking up multispeaker videos into "who spoke when". Based on this work we are hosting a new <i>speaker diarisation</i> track 
                at the <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/competition.html", style="color: orange"> VoxCeleb Speaker Recognition Challenge. </a></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/weakly_sup_ad-1.png" alt="disent" width="160" height="110">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2007.10703">
                <papertitle> Uncertainty-Aware Weakly Supervised Action Detection from Untrimmed Videos </papertitle>
              </a>
              <br>
               Anurag Arnab, Chen Sun, Arsha Nagrani, Cordelia Schmid 
               
  
              <br>
              <em>ECCV</em>, 2020 
              <br>
              <a href="https://arxiv.org/abs/2007.10703">PDF</a> /
              <a href="data/Arnab20.bib">bibtex</a>
              <p></p>
              <p>Action localisation in movies using video level labels only.</p>
            </td>
          </tr>
          
          
              
              <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/s2a.gif" alt="disent" width="160" height="110">
            </td>
            <td width="75%" valign="middle">
              <a href="http://www.robots.ox.ac.uk/~vgg/publications/2020/Nagrani20/nagrani20.pdf">
                <papertitle> Speech2Action: Cross-modal Supervision for Action Recognition </papertitle>
              </a>
              <br>
               Arsha Nagrani, Chen Sun, David Ross, Rahul Sukthankar, Cordelia Schmid, Andrew Zisserman 
              <br>
              <em>CVPR</em>, 2020 
              <br>
              <a href="https://www.robots.ox.ac.uk/~vgg/research/speech2action/">project page, data</a> /
              <a href="https://docs.google.com/presentation/d/1ZJLwtZ1xt9OTtPD2NmzucVF-Ymk3vG7eCEd8EcQQ_-8/edit?usp=sharing">slides</a> /
              <a href="data/Nagrani20c.bib">bibtex</a>
              <p></p>
              <p>Action recognition in movies using the speech alone.</p>
            </td>
          </tr>

            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/disentangled.png" alt="disent" width="170" height="150">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2002.08742">
                    <papertitle>Disentangled Speech Embeddings using Cross-modal Self-supervision </papertitle>
                  </a>
                  <br>
             Arsha Nagrani*, Joon Son Chung*, Samuel Albanie*, Andrew Zisserman 
                  <br>
                  <em>ICASSP</em>, 2020 
                  <br>
                  <a href="https://www.robots.ox.ac.uk/~vgg/research/cross-modal-disentanglement/">project page</a> /
            <a href="https://github.com/joonson/syncnet_trainer">some code</a> /
                  <a href="data/Nagrani20b.bib">bibtex</a>
                  <p></p>
                  <p>Disentanglement of speech embeddings into content and identity with only accompanying facetrack as supervision. Based on this work we are hosting a new <i>self-supervised</i> track 
                    at the <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/competition.html", style="color: orange"> VoxCeleb Speaker Recognition Challenge. </a>
                  </p>
                </td>
              </tr>


            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/spID.png" alt="vox" width="170" height="150">
                </td>
                <td width="75%" valign="middle">
                  <a href="http://www.robots.ox.ac.uk/~vgg/publications/2019/Nagrani19/nagrani19.pdf">
                    <papertitle>Voxceleb: Large-scale speaker verification in the wild </papertitle>
                  </a>
                  <br>
            Arsha Nagrani, Joon Son Chung, Weidi Xie, Andrew Zisserman 
                  <br>
                  <em>Computer Speech and Language</em>, 2020 
                  <br>
            <a href="http://www.robots.ox.ac.uk/~vgg/research/speakerID/">project page, data </a> /
                  <a href="https://github.com/WeidiXie/VGG-Speaker-Recognition">code & models</a> /
                  <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/competition.html", style="color: orange"> challenge</a> /
                  <a href="data/Nagrani20.bib">bibtex</a>
                  <p></p>
                  <p>Overview of the VoxCeleb1 and VoxCeleb2 datasets including various updates and splits, and new models for speaker recognition.</p>
                </td>
              </tr>

            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/CCR.gif" alt="ccr" width="170" height="130">
                </td>
                <td width="75%" valign="middle">
                  <a href="http://www.robots.ox.ac.uk/~vgg/publications/2019/Bain19/bain19.pdf">
                    <papertitle>Count, Crop and Recognise: Fine-Grained Recognition in the Wild  </papertitle>
                  </a>
                  <br> Max Bain, Arsha Nagrani, Daniel Schofield, Andrew Zisserman
                  <br>
                  <em>ICCV Workshops</em>, 2019 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                  <br>
                  <a href="http://www.robots.ox.ac.uk/~vgg/research/ccr/">project page</a> / <a href="data/Bain19.bib">bibtex</a> / <a href="http://www.robots.ox.ac.uk/~vgg/research/ccr/files/ICCV_Pres.pptx">slides</a>
                </br>
          
                  <p></p>
                  <p>Recognition of wild chimpanzees using full body and full frame CNNs methods. We also release an 'in the wild' video chimpanzee recognition dataset. 	  
            </p>
                </td>
              </tr>
              
              
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/chimpanzees.gif" alt="chimpanzees-facial-recognition" width="170" height="130">
            </td>
            <td width="75%" valign="middle">
              <a href="https://advances.sciencemag.org/content/5/9/eaaw0736">
                <papertitle>Chimpanzee face recognition from videos in the wild using deep learning</papertitle>
              </a>
              <br> Daniel Schofield*, Arsha Nagrani*, Andrew Zisserman, Misato Hayashi, Tetsuro Matsuzawa, Dora Biro, Susana Carvalho
              <br>
              <em>Science Advances</em>, 2019
              <br>
              <a href="https://www.robots.ox.ac.uk/~vgg/research/ChimpanzeeFaces/">project page</a> / <a href="data/Schofield19.bib">bibtex</a>
            </br>
              
		      Press:
		      <font color="orange">
				  <a href="https://www.newscientist.com/article/2215359-ai-facial-recognition-software-now-works-for-wild-chimpanzees-too/", style="color: orange">New Scientist</a>,
				  <a href="https://www.technologyreview.com/f/614260/ai-face-recognition-tracks-chimpanzees/", style="color: orange">MIT Tech Review</a>, 
				  <a href="https://techxplore.com/news/2019-09-artificial-intelligence-primate-wild.html", style="color: orange">TechXplore</a>, 
				  <a href="https://www.verdict.co.uk/chimpanzees-facial-recognition/", style="color: orange">Verdict</a>, 
				  <a href="https://www.digitaltrends.com/cool-tech/facial-recognition-chimpanzee/", style="color: orange">Digital Trends</a>, 
				  <a href="https://www.eng.ox.ac.uk/news/artificial-intelligence-used-to-recognise-primate-faces-in-the-wild/", style="color: orange">Oxford News</a> 
			  </font>
			  <br>
              <p></p>
              <p>Face detection, tracking, and recognition of wild chimpanzees from long-term video records using deep CNNs. We also show a brief application for social network analysis.
			  </p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/TBW.png" alt="clean-usnob" width="170" height="140">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/1908.08498.pdf">
                <papertitle>EPIC-Fusion: Audio-Visual Temporal Binding for Egocentric Action Recognition</papertitle>
              </a>
              <br>
			  Evangelos Kazakos, Arsha Nagrani, Andrew Zisserman, Dima Damen
              <br>
              <em>ICCV</em>, 2019
              <br>
			  <a href="https://ekazakos.github.io/TBN/">project page</a> /
              <a href="https://www.youtube.com/watch?time_continue=177&v=VzoaKsDvv1o">video</a> /
              <a href="https://github.com/ekazakos/temporal-binding-network">code and models</a> /
              <a href="data/Kazakos19.bib">bibtex</a>
              <br>
              <p></p>
              <p>We propose a novel architecture for combining modalities in videos for action recognition, by using a temporal window to allow a range of temporal offsets.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/CE.png" alt="clean-usnob" width="170" height="150">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/1907.13487.pdf">
                <papertitle>Use What You Have: Video Retrieval Using Representations From Collaborative Experts</papertitle>
              </a>
              <br>
              Yang Liu*, Samuel Albanie*, Arsha Nagrani*, Andrew Zisserman
              <br>
              <em>BMVC</em>, 2019
              <br>
			  <a href="https://www.robots.ox.ac.uk/~vgg/research/collaborative-experts/">project page</a> /
              <a href="https://github.com/albanie/collaborative-experts">code & models</a> /
              <a href="https://www.robots.ox.ac.uk/~vgg/challenges/video-pentathlon/", style="color: orange"> challenge</a> /
              <a href="data/Liu19a.bib">bibtex</a> 

              <p></p>
              <p>We fuse the information from different embeddings experts for the task of video retrieval - achieving SOTA results on 5 different datasets. This work is also the basis 
                for the <a href="https://www.robots.ox.ac.uk/~vgg/challenges/video-pentathlon/">Video Pentathlon</a> at CVPR 2020. 
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/spID.png" alt="clean-usnob" width="170" height="150">
            </td>
            <td width="75%" valign="middle">
              <a href="http://www.robots.ox.ac.uk/~vgg/publications/2019/Xie19a/xie19a.pdf">
                <papertitle>Utterance-level Aggregation For Speaker Recognition In The Wild  </papertitle>
              </a>
              <br>
			  Weidi Xie, Arsha Nagrani, Joon Son Chung, Andrew Zisserman 
              <br>
              <em>ICASSP</em>, 2019 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
			  <a href="http://www.robots.ox.ac.uk/~vgg/research/speakerID/">project page</a> /
              <a href="https://github.com/WeidiXie/VGG-Speaker-Recognition">code & models</a> /
              <a href="data/Xie19a.bib">bibtex</a>
              <p></p>
              <p>A NetVlad layer in a deep CNN works well for speaker recognition on long noisy speech utterances.</p>
            </td>
          </tr>

          <tr onmouseout="acm_stop()" onmouseover="acm_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='acm_image'><img src='images/acm-after.png' alt="clean-usnob" width="170" height="150"></div>
                <img src='images/acm-before.png' alt="clean-usnob" width="170" height="150">
              </div>
              <script type="text/javascript">
                function acm_start() {
                  document.getElementById('acm_image').style.opacity = "1";
                }

                function acm_stop() {
                  document.getElementById('acm_image').style.opacity = "0";
                }
                acm_stop()
              </script>
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/1808.05561.pdf">
                <papertitle>Emotion Recognition in Speech using Cross-Modal Transfer in the Wild  </papertitle>
              </a>
              <br>
			  Samuel Albanie*, Arsha Nagrani*, Andrea Vedaldi, Andrew Zisserman
              <br>
              <em>ACM Multimedia</em>, 2018 
              <br>
              <a href="http://www.robots.ox.ac.uk/~vgg/research/cross-modal-emotions/">project page</a> /
              <a href="https://github.com/albanie/mcnCrossModalEmotions">code</a> /
              <a href="data/Albanie18.bib">bibtex</a>
              <p></p>
              <p>We use the redundant (common) signal in both audio (speech) and vision (faces) to learn speech representations for emotion recognition without manual supervision.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/vox2.png" alt="safs_small" width="170" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/1806.05622.pdf">
                <papertitle>VoxCeleb2: Deep Speaker Recognition </papertitle>
              </a>
              <br>
              Joon Son Chung*, Arsha Nagrani*, Andrew Zisserman 
              <br>
              <em>INTERSPEECH</em>, 2018
              <br>
			  <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/">data</a> / 
			  <a href="data/Chung18a.bib">bibtex</a>
              <p>Speaker Recognition in the Wild using deep CNNs. The VoxCeleb datasets are also used integrally in the 
				  <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/competition.html", style="color: orange"> VoxCeleb Speaker Recognition Challenge. </a></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/jointembedding.png" alt="fast-texture" width="170" height="120">
            </td>
            <td width="75%" valign="middle">
              <a href="http://www.robots.ox.ac.uk/~vgg/publications/2018/Nagrani18c/nagrani18c.pdf">
                <papertitle>Learnable PINs: Cross-Modal Embeddings for Person Identity  </papertitle>
              </a>
              <br>
              Arsha Nagrani*, Samuel Albanie*, Andrew Zisserman 
              <br>
              <em>ECCV</em>, 2018
              <br>
              <a href="http://www.robots.ox.ac.uk/~vgg/research/LearnablePins/">project page</a> / 			  
			  <a href="data/Nagrani18c.bib">bibtex</a>
              <p>We learn joint embedding of faces and voices using cross-modal self-supervision from YouTube videos. 
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/seeingvoices.gif" alt="prl" width="170" height="120">
            </td>
            <td width="75%" valign="middle">
              <a href="http://www.robots.ox.ac.uk/~vgg/publications/2018/Nagrani18a/nagrani18a.pdf">
                <papertitle>Seeing Voices and Hearing Faces: Cross-modal biometric matching  </papertitle>
              </a>
              <br>
              Arsha Nagrani, Samuel Albanie, Andrew Zisserman
              <br>
              <em>CVPR</em>, 2018 &nbsp <font color="red"><strong>(Spotlight)</strong></font>
              <br>
              <a href="http://www.robots.ox.ac.uk/~vgg/research/CMBiometrics/">project page</a> / 
			  <a href="https://www.youtube.com/watch?time_continue=4&v=AJt993-VGsk"> video </a> / 
			  <a href="http://www.robots.ox.ac.uk/~vgg/blog/seeing-voices-and-hearing-faces.html"> blog post </a> / 			  
			  <a href="data/Nagrani18a.bib">bibtex</a>
              <p> Can you recognise someone’s face if you have only heard their voice? Or recognise their voice if you have only seen their face? </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/sherlock.gif" alt="blind-date" width="170" height="110">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.robots.ox.ac.uk/~vgg/publications/2017/Nagrani17b/nagrani17b.pdf">
                <papertitle>From Benedict Cumberbatch to Sherlock Holmes: Character Identification in TV series without a Script  </papertitle>
              </a>
              <br>
              Arsha Nagrani, Andrew Zisserman
              <br>
              <em>BMVC</em>, 2017 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
			  <br>
			  <a href="http://www.robots.ox.ac.uk/~vgg/research/Sherlock/">project page</a> / 
			  <a href="data/Nagrani17b.bib">bibtex</a>
              <p></p>
            </td>
          </tr>

	   <tr onmouseout="vox1_stop()" onmouseover="vox1_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='vox1_image'><img src='images/vox1-after.png' alt="clean-usnob" width="170" height="145"></div>
                <img src='images/voxceleb1.png' alt="clean-usnob" width="170" height="145">
              </div>
              <script type="text/javascript">
                function vox1_start() {
                  document.getElementById('vox1_image').style.opacity = "1";
                }

                function vox1_stop() {
                  document.getElementById('vox1_image').style.opacity = "0";
                }
                vox1_stop()
              </script>
            </td>
            <td width="75%" valign="middle">
              <a href="http://www.robots.ox.ac.uk/~vgg/publications/2017/Nagrani17/nagrani17.pdf">
                <papertitle>VoxCeleb: a large-scale speaker identification dataset  </papertitle>
              </a>
              <br>
              Arsha Nagrani*, Joon Son Chung*, Andrew Zisserman
              <br>
              <em>INTERSPEECH</em>, 2017 &nbsp <font color="red"><strong>(Oral Presentation, Best Student Paper Award)</strong></font>
			  <br>
			  <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/">data</a> / 
			  <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/competition.html">challenge</a> / 
			  <a href="data/Nagrani17.bib">bibtex</a>
              <p>We use face recognition and active speaker detection to automatically create a large scale speaker identification dataset from YouTube videos.</p>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
        <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Teaching/Invited Talks</heading>
            <p>
            "Multimodality for Video Understanding", <a href="https://sites.google.com/view/aisummerschool2020">Google Research India AI Summer School</a>, 2020 [<a href="https://docs.google.com/presentation/d/1J4AVI9S1KBFZyGiBdFJn64k2BRFhlRRngd8r262Xo8g/edit?usp=sharing">slides</a>] </br>
            "Learning joint representations for visual and language tasks", <a href="https://multimodal-knowledge-discovery.github.io/">Online Multimodal Knowledge Discovery Tutorial</a>, ICDM 2020
            "Applications of Machine Learning", Oxford University MPLS DTC on Statistics and Data Mining, 2020 [<a href="https://docs.google.com/presentation/d/1XgNYFLNwlODst7LiFuaof2F9uuNcv3ZdwdiyXrCT1-A/edit?usp=sharing">slides</a>]
            </p>
          </td>
        </tr>
      </tbody></table>
    





        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <th style="padding:20px;width:25%;vertical-align:middle;text-align: center">
                      <img src="images/video-pent-logo.svg" alt="video-pent" width="100" height="80" class="center">
            </th>
           <td width="75%" valign="center">
            <strong> The End-of-End-to-End: </strong>  A Video Understanding Pentathlon @ CVPR 2020
            </br>
            <a href="https://arxiv.org/pdf/2008.00744.pdf">report</a> /
            <a href="https://www.robots.ox.ac.uk/~vgg/challenges/video-pentathlon/challenge.html">challenge</a> /
            <a href="https://www.robots.ox.ac.uk/~vgg/challenges/video-pentathlon/">workshop</a> 	/
            <a href="https://www.youtube.com/watch?v=L2rff2mu1gE">recording</a> 	
            </br>										 
            </br>
    
            </td>
            </tr>

          <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/voxsrc.png" alt="voxsrc" width="150" height="50">
        </td>
       <td width="75%" valign="center">
	      <strong> VoxSRC: </strong> VoxCeleb Speaker Recognition Challenge @ INTERSPEECH 
        </br>
        [2020]
        <a href="https://arxiv.org/pdf/2012.06867.pdf">report</a> /
	      <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/competition2020.html">challenge</a> /
	      <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/interspeech2020.html">workshop</a> /
        <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/index.html">data</a>													              
       </br>										 
        [2019]
        <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/files/VoxSRC19.pdf">report</a> /
	      <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/competition2019.html">challenge</a> /
	      <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/interspeech2019.html">workshop</a> /
	      <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/index.html">data</a>													               </br>										 
	      </br>

        </td>
        </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/wicv.png" alt="wicv" width="150" height="50">
      </td>
      <td width="75%" valign="center">
        <strong> WICV: </strong> Women in Computer Vision Workshop @ CVPR
      </br>
      [2020]
      <a href="https://sites.google.com/view/wicvworkshop-cvpr2020/">website</a> /
      <a href="https://twitter.com/wicvworkshop?lang=en">twitter</a>													     									 
      </br>
        [2019]
	      <a href="http://openaccess.thecvf.com/content_CVPRW_2019/html/WiCV/Amerini_WiCV_2019_The_Sixth_Women_In_Computer_Vision_Workshop_CVPRW_2019_paper.html">report</a> /
	      <a href="https://wicvworkshop.github.io/CVPR2019/index.html">website</a> /
	      <a href="https://twitter.com/wicvworkshop?lang=en">twitter</a>													         </br>										 
        </br>
        </td>
        </tr>
        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle;text-align: center">
                <img src="images/reviewer.jpg" alt="review" width="150" height="70" >
      </td>
          <td>
              <strong> Reviewer </strong>: CVPR, ECCV, ICCV, BMVC, NeurIps, ICML, AAAI, IEEE Triple Access
              <br>

            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                 <a href="https://jonbarron.info/">This</a> guy is good at website design.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
